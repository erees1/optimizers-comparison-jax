# Optimizer comparison for deep learning using Jax

Purpose - understanding differences in implementation and performance of optimizers. Optimizers are implemented from scrach using Jax and are tested on MNIST and CIFAR-10. Some code borrowed from the JAX example pages.
* SGD
* SGD with Momentum

Results [here](./results.ipynb)
